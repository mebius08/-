{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nfrom PIL import Image\nimport numpy as np\nfrom typing import List, Dict, Any, Union, Optional\nimport os\nimport cv2  # OpenCV для чтения видео\nimport decord  # гораздо быстрее OpenCV для больших видео (рекомендую установить: pip install decord)\n\n\n# =====================================================\n# 1. Основной мультимодальный классификатор (изображения + видео)\n# =====================================================\nclass MultimodalVideoTemplateClassifier(nn.Module):\n    def __init__(\n        self,\n        clip_model_name: str = \"openai/clip-vit-large-patch14-336\",  # 336px версия чуть точнее для видео\n        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n        normalize: bool = True,\n        num_frames: int = 8,                 # сколько кадров семплировать из видео\n        sampling_strategy: str = \"uniform\",  # \"uniform\" или \"sparse\" (как в оригинальном CLIP)\n    ):\n        super().__init__()\n        \n        self.clip_model = CLIPModel.from_pretrained(clip_model_name)\n        self.processor = CLIPProcessor.from_pretrained(clip_model_name)\n        \n        self.device = device\n        self.normalize = normalize\n        self.num_frames = num_frames\n        self.sampling_strategy = sampling_strategy\n        \n        self.clip_model.to(device)\n        self.clip_model.eval()\n        \n        # Кэш текстовых эмбеддингов\n        self.text_emb_cache: Dict[str, torch.Tensor] = {}\n\n    # =====================================================\n    # 2. Чтение и семплинг кадров из видео\n    # =====================================================\n    def _read_video_decord(self, video_path: str) -> torch.Tensor:\n        \"\"\"\n        Самый быстрый и надёжный способ — decord (работает на GPU если нужно)\n        Возвращает tensor [T, H, W, C] в uint8\n        \"\"\"\n        decord.bridge.set_bridge('torch')  # возвращаем сразу torch tensor\n        vr = decord.VideoReader(video_path, ctx=decord.cpu(0))\n        total_frames = len(vr)\n        \n        if total_frames <= self.num_frames:\n            indices = np.arange(0, total_frames)\n        else:\n            if self.sampling_strategy == \"uniform\":\n                indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n            elif self.sampling_strategy == \"sparse\":  # как в оригинальных видео-CLIP экспериментах\n                step = total_frames // self.num_frames\n                indices = np.arange(0, total_frames, step)[:self.num_frames]\n            else:\n                raise ValueError(\"sampling_strategy должен быть 'uniform' или 'sparse'\")\n        \n        frames = vr.get_batch(indices)  # [T, H, W, C] uint8\n        frames = frames.permute(0, 3, 1, 2)  # [T, C, H, W] для CLIP\n        return frames\n\n    def _read_video_opencv(self, video_path: str) -> List[Image.Image]:\n        \"\"\"\n        Резервный вариант через OpenCV, если decord не установлен\n        \"\"\"\n        cap = cv2.VideoCapture(video_path)\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        \n        indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n        frames = []\n        \n        for idx in indices:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n            ret, frame = cap.read()\n            if not ret:\n                continue\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(Image.fromarray(frame))\n        \n        cap.release()\n        return frames if len(frames) > 0 else [Image.new(\"RGB\", (336, 336), (0, 0, 0))]\n\n    # =====================================================\n    # 3. Получение эмбеддинга изображения ИЛИ видео\n    # =====================================================\n    @torch.no_grad()\n    def encode_media(\n        self,\n        media_input: Union[str, Image.Image, List[Union[str, Image.Image]]]\n    ) -> torch.Tensor:\n        \"\"\"\n        Универсальная функция: на входе может быть\n        - путь к картинке / видео\n        - PIL.Image\n        - список из вышеперечисленного\n        Возвращает нормализованный эмбеддинг [1, dim] (усреднённый по кадрам если видео)\n        \"\"\"\n        # 1. Приводим всё к списку PIL.Image\n        pil_frames: List[Image.Image] = []\n        \n        if isinstance(media_input, (str, Image.Image)):\n            media_input = [media_input]\n        \n        for item in media_input:\n            if isinstance(item, Image.Image):\n                pil_frames.append(item.convert(\"RGB\"))\n            elif isinstance(item, str):\n                if os.path.isfile(item):\n                    # Пробуем определить — видео или картинка по расширению\n                    video_exts = {\".mp4\", \".avi\", \".mov\", \".mkv\", \".webm\", \".gif\"}\n                    _, ext = os.path.splitext(item.lower())\n                    if ext in video_exts:\n                        # Это видео → семплируем кадры\n                        try:\n                            video_tensor = self._read_video_decord(item)  # [T, C, H, W]\n                            for i in range(video_tensor.shape[0]):\n                                frame = video_tensor[i].permute(1, 2, 0).cpu().numpy()  # [H,W,C]\n                                frame = np.clip(frame, 0, 255).astype(np.uint8)\n                                pil_frames.append(Image.fromarray(frame))\n                        except Exception as e:\n                            print(f\"decord не сработал, пробуем OpenCV: {e}\")\n                            pil_frames.extend(self._read_video_opencv(item))\n                    else:\n                        # Обычная картинка\n                        pil_frames.append(Image.open(item).convert(\"RGB\"))\n                else:\n                    raise FileNotFoundError(f\"Файл не найден: {item}\")\n        \n        # 2. Если кадров слишком много — можно дополнительно субсемплировать\n        if len(pil_frames) > self.num_frames * 2:\n            step = len(pil_frames) // self.num_frames\n            pil_frames = pil_frames[::step][:self.num_frames]\n        \n        # 3. Прогоняем через CLIP\n        inputs = self.processor(\n            images=pil_frames,\n            return_tensors=\"pt\",\n            padding=True,\n        ).to(self.device)\n        \n        image_features = self.clip_model.get_image_features(**inputs)  # [N_frames, dim]\n        \n        if self.normalize:\n            image_features = F.normalize(image_features, p=2, dim=-1)\n        \n        # Усредняем по всем кадрам\n        media_embedding = image_features.mean(dim=0, keepdim=True)  # [1, dim]\n        return media_embedding\n\n    # =====================================================\n    # 4. Кодирование текстовых шаблонов (то же самое)\n    # =====================================================\n    def encode_text_templates(\n        self,\n        class_names: List[str],\n        templates: List[str] = None,\n    ) -> torch.Tensor:\n        # (тот же код, что и в предыдущей версии)\n        if templates is None:\n            templates = [\n                \"a photo of a {}.\",\n                \"a video of a {}.\",\n                \"this is a {}.\",\n                \"an image showing {}.\",\n                \"{}.\",\n            ]\n        \n        text_inputs_list = []\n        for class_name in class_names:\n            for tmpl in templates:\n                text_inputs_list.append(tmpl.format(class_name))\n        \n        inputs = self.processor(\n            text=text_inputs_list,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n        ).to(self.device)\n        \n        with torch.no_grad():\n            text_features = self.clip_model.get_text_features(**inputs)\n            \n        if self.normalize:\n            text_features = F.normalize(text_features, p=2, dim=-1)\n        \n        num_templates = len(templates)\n        text_features = text_features.view(len(class_names), num_templates, -1).mean(dim=1)\n        return text_features\n\n    # =====================================================\n    # 5. Основной метод предсказания\n    # =====================================================\n    @torch.no_grad()\n    def predict(\n        self,\n        media_input: Union[str, Image.Image, List[Union[str, Image.Image]]],\n        class_names: List[str],\n        templates: Optional[List[str]] = None,\n        temperature: float = 0.07,\n        return_probs: bool = True,\n    ) -> Dict[str, Any]:\n        \n        # 1. Эмбеддинг медиа (картинка или видео)\n        media_emb = self.encode_media(media_input)  # [1, dim]\n        \n        # 2. Текстовые эмбеддинги (кэшируем)\n        cache_key = \"|\".join(class_names) + \"||\" + (\"|\".join(templates or [\"default\"]))\n        if cache_key not in self.text_emb_cache:\n            self.text_emb_cache[cache_key] = self.encode_text_templates(class_names, templates)\n        \n        text_embs = self.text_emb_cache[cache_key]  # [num_classes, dim]\n        \n        # 3. Косинусное сходство\n        logits = media_emb @ text_embs.T  # [1, num_classes]\n        \n        if return_probs:\n            probs = torch.softmax(logits / temperature, dim=-1)\n        else:\n            probs = None\n        \n        pred_idx = logits.argmax(dim=-1).item()\n        pred_class = class_names[pred_idx]\n        \n        return {\n            \"logits\": logits.squeeze(0).cpu(),\n            \"probs\": probs.squeeze(0).cpu() if probs is not None else None,\n            \"predicted_idx\": pred_idx,\n            \"predicted_class\": pred_class,\n            \"class_names\": class_names,\n        }\n\n\n# =====================================================\n# 6. Пример использования с видео\n# =====================================================\nif __name__ == \"__main__\":\n    classifier = MultimodalVideoTemplateClassifier(\n        clip_model_name=\"openai/clip-vit-large-patch14-336\",\n        num_frames=12,          # можно 8–16, больше — точнее, но медленнее\n        sampling_strategy=\"uniform\",\n    )\n    \n    class_names = [\n        \"кошка играет\",\n        \"собака бегает\",\n        \"человек танцует\",\n        \"автомобиль едет\",\n        \"готовка еды\",\n        \"спорт\",\n        \"природа и пейзаж\",\n        \"мультфильм\",\n    ]\n    \n    # Можно передать и картинку, и видео, и список\n    result = classifier.predict(\n        media_input=\"video_dancing_cat.mp4\",  # ← твоё видео\n        class_names=class_names,\n        # templates=[\"это видео про {}\", \"видео с {}\"],\n        temperature=0.1,\n    )\n    \n    print(\"Предсказанный класс:\", result[\"predicted_class\"])\n    print(\"\\nВсе вероятности:\")\n    for name, prob in zip(class_names, result[\"probs\"]):\n        print(f\"  {name}: {prob:.4f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}